References:
================
http://spark.apache.org/docs/latest/quick-start.html
http://scala-ide.org
winutils.exe - to disguise spark about hadoop
grouplens.org - for sample data
http://media.sundog-soft.com/SparkScala/SparkScala.zip  --- scala samples  from udemy
 
Q&A
=========


Key Points & Buzz words
=====================
10 times and 100 times faster due to optimized workflow of  DAG Engine - Directed Acyclic Graph
RDD - Resilient Distributed Dataset
Spark itself is written in Scala
Scala's functional programming model is a good fit for distributed processing

Creating RDDs
 val nums = parallelzie(List(1,2,3,4))
 sc.textFile("file:///c:/user.txt")  or s3n:// , hdfs://
 hiveCtx = HiveContext(sc)
     rows = hiveCtx.sql("select name, age from users")
Can also create from:
 JDBC
 Cassandra
 HBase
 Elastisearch
 JSON,CSV,sequence files, object files, various compressed formats

RDD Actions
  collect
  count
  countByValue
  take
  top
  reduce
  ...
map vs flatMap
NLTK - Natural Language Toolkit

sc.broadcast() -- to ship off whatever you want
sc.value() to get the Object back

Item based collaborative filtering

cache vs persist - to store rdd in-memory or not to re-build

running spark program on cluster
spark-submit -class<full package with class>  --jars <dependent jars> --files <files you want to be with your prog> <actual jar file>

--- SPARK SQL --
SparkSession

DataFrame 
 - Contain Row(DataSet) objects
 - Can run SQL queries
 - Has a schema (leading to more efficient storage)
 - Read and write to JSON , Hive, parquet
 - Communicates with JDBC/ODBC, Tableau

import spark.implicits._
val movieDS = linesRdd.toDS()  ---> this converts the rdd to dataset



----- SPARK MLLIB------
MLLIB Capabilities
 Feature extraction
 Basic statistics
 Linear regression, logistic regression 
 support vector Machines
 Naive Bayes classifier
 Decision tress
 K-means clustering
 Principal componentt analysis, singular value decomposition
 Recommendations using Alternating Least Squares

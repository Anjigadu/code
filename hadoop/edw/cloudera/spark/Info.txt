References:
================
http://spark.apache.org/docs/latest/quick-start.html
http://scala-ide.org
winutils.exe - to disguise spark about hadoop
grouplens.org - for sample data
http://media.sundog-soft.com/SparkScala/SparkScala.zip  --- scala samples  from udemy
 
Q&A
=========


Key Points & Buzz words
=====================
               RDD              Vs       DSM (Distributed Shared Memory)
  ----------------------------------------------
Writes ->    Coarse grained             Fine grained
Recovery->    Lineage                   Check Point
Consistency--> Trivial                  Up to the application
  
Benefits of Spark
------------------
1. Fault Recovery
2. Optimized (using DAG)
3. Easy Programming
4. Rich Library/Support (MLIB, GraphX, DataFrames)




10 times and 100 times faster due to optimized workflow of  DAG Engine - Directed Acyclic Graph
RDD - Resilient Distributed Dataset
Spark itself is written in Scala
Scala's functional programming model is a good fit for distributed processing

Creating RDDs
-------
     val nums = parallelzie(List(1,2,3,4))
     sc.textFile("file:///c:/user.txt")  or s3n:// , hdfs://
     hiveCtx = HiveContext(sc)
         rows = hiveCtx.sql("select name, age from users")
    Can also create from:
     JDBC
     Cassandra
     HBase
     Elastisearch
     JSON,CSV,sequence files, object files, various compressed formats

RDD Actions
  collect
  count
  countByValue
  take
  top
  reduce
  ...
map vs flatMap
NLTK - Natural Language Toolkit

sc.broadcast() -- to ship off whatever you want
sc.value() to get the Object back

Item based collaborative filtering

cache vs persist - to store rdd in-memory or not to re-build

running spark program on cluster
spark-submit -class<full package with class>  --jars <dependent jars> --files <files you want to be with your prog> <actual jar file>

--- SPARK SQL --
SparkSession

DataFrame 
 - Contain Row(DataSet) objects
 - Can run SQL queries
 - Has a schema (leading to more efficient storage)
 - Read and write to JSON , Hive, parquet
 - Communicates with JDBC/ODBC, Tableau

import spark.implicits._
val movieDS = linesRdd.toDS()  ---> this converts the rdd to dataset



----- SPARK MLLIB------
MLLIB Capabilities
 Feature extraction
 Basic statistics
 Linear regression, logistic regression 
 support vector Machines
 Naive Bayes classifier
 Decision tress
 K-means clustering
 Principal componentt analysis, singular value decomposition
 Recommendations using Alternating Least Squares



---- SPARK STREAMING ----
Analyzes continual streams of data
Data is aggregated and analyzed at some given interval
Can take data fed to some port, Amazon Kinesis , HDFS, Kafka, Flume and others.
Checkpointing - stores state to disk periodically for fault tolerance.

DStream - collection of distinct RDDs
val stream = new StreamingContext(conf,Seconds(1))
val lines = stream.socketTextStream("localhost",8888)
val errors = lines.filter(_.contains("error"))

kick off the job explicitly :
 stream.start()
 stream.awaitTermination()

Windowed Opertaions
updateStateByKey()

Need a twitter developer account  --- https://apps.twitter.com/

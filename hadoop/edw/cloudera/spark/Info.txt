References:
================
http://spark.apache.org/docs/latest/quick-start.html
http://scala-ide.org
winutils.exe - to disguise spark about hadoop
grouplens.org - for sample data
http://media.sundog-soft.com/SparkScala/SparkScala.zip  --- scala samples  from udemy
RDD vs DataFrames vs DataSets
   https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html
Spark Job submit 
   http://spark.apache.org/docs/latest/submitting-applications.html
   
   
Q&A
=========

01. Which Storage Level to Choose?
     http://spark.apache.org/docs/latest/programming-guide.html#which-storage-level-to-choose

02. Spark - repartition() vs coalesce()
     http://stackoverflow.com/questions/31610971/spark-repartition-vs-coalesce
03. 

Key Points & Buzz words
=====================

When to use RDDs?
Consider these scenarios or common use cases for using RDDs when:
      you want low-level transformation and actions and control on your dataset;
      your data is unstructured, such as media streams or streams of text;
      you want to manipulate your data with functional programming constructs than domain specific expressions;
      you don’t care about imposing a schema, such as columnar format, while processing or accessing data attributes by name or column; and
      you can forgo some optimization and performance benefits available with DataFrames and Datasets for structured and semi-structured data
      
      
When should I use DataFrames or Datasets?
      If you want rich semantics, high-level abstractions, and domain specific APIs, use DataFrame or Dataset.
      If your processing demands high-level expressions, filters, maps, aggregation, averages, sum, SQL queries, columnar access and use of lambda functions on semi-structured data, use DataFrame or Dataset.
      If you want higher degree of type-safety at compile time, want typed JVM objects, take advantage of Catalyst optimization, and benefit from Tungsten’s efficient code generation, use Dataset.
      If you want unification and simplification of APIs across Spark Libraries, use DataFrame or Dataset.
      If you are a R user, use DataFrames.
      If you are a Python user, use DataFrames and resort back to RDDs if you need more control.
      Note that you can always seamlessly interoperate or convert from DataFrame and/or Dataset to an RDD, by simple method call .rdd. For instance,


Benefits of Dataset APIs
   1. Static-typing and runtime type-safety
   2. High-level abstraction and custom view into structured and semi-structured data
   3. Ease-of-use of APIs with structure
   4. Performance and Optimization
   



               RDD              Vs       DSM (Distributed Shared Memory)
  ----------------------------------------------
Writes ->    Coarse grained             Fine grained
Recovery->    Lineage                   Check Point
Consistency--> Trivial                  Up to the application
  
Benefits of Spark
------------------
1. Fault Recovery
2. Optimized (using DAG)
3. Easy Programming
4. Rich Library/Support (MLIB, GraphX, DataFrames)




10 times and 100 times faster due to optimized workflow of  DAG Engine - Directed Acyclic Graph
RDD - Resilient Distributed Dataset
Spark itself is written in Scala
Scala's functional programming model is a good fit for distributed processing

Creating RDDs
-------
     val nums = parallelzie(List(1,2,3,4))
     sc.textFile("file:///c:/user.txt")  or s3n:// , hdfs://
     hiveCtx = HiveContext(sc)
         rows = hiveCtx.sql("select name, age from users")
    Can also create from:
     JDBC
     Cassandra
     HBase
     Elastisearch
     JSON,CSV,sequence files, object files, various compressed formats

RDD Actions
  collect
  count
  countByValue
  take
  top
  reduce
  ...
map vs flatMap
NLTK - Natural Language Toolkit


Shared Variables
   1. BroadCast
         val broadcastVar = sc.broadcast(Array(1, 2, 3))
        broadcastVar.value   -----  to get the Object back
    2. Accumulators
         val accum = sc.longAccumulator("My Accumulator")
         sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum.add(x))
         accum.value   ---- 10 is returned to driver program only. the rdds can't access the accumulator.

rdd.mapValues(x=>(x,1))
rdd.reduceByValues((x,y)=>(x._1+y._1,x._2+y._2))
parsedLinesrdd.filter(x=> x._2=="TMIN") // filtering min temp lines in rdd.
linesrdd.flatMap(x=>x.split(" "))



Item based collaborative filtering

cache vs persist 
    - to store rdd in-memory or not to re-build. Cache is same as persist with default storage level MEMORY_ONLY.
    - while with persist rdd can be stored in memory and disk. Different levels of persistence are MEMORY_ONLY, MEMORY_ONLY_SER, 
      MEMORY_AND_DISK, MEMORY_AND_DISK_SER, DISK_ONLY.
Removing Data 
      RDD.unpersist() do it intentionally to remove partition; or wait for spark to remove it based on LRU (Least Recently Used)

running spark program on cluster
spark-submit -class<full package with class>  --jars <dependent jars> --files <files you want to be with your prog> <actual jar file>

--- SPARK SQL --
SparkSession

DataFrame 
 - Contains Row(DataSet) objects
 - Can run SQL queries
 - Has a schema (leading to more efficient storage)
 - Read and write to JSON , Hive, parquet
 - Communicates with JDBC/ODBC, Tableau

import spark.implicits._
val movieDS = linesRdd.toDS()  ---> this converts the rdd to dataset



----- SPARK MLLIB------
MLLIB Capabilities
 Feature extraction
 Basic statistics
 Linear regression, logistic regression 
 support vector Machines
 Naive Bayes classifier
 Decision tress
 K-means clustering
 Principal componentt analysis, singular value decomposition
 Recommendations using Alternating Least Squares



---- SPARK STREAMING ----
Analyzes continual streams of data
Data is aggregated and analyzed at some given interval
Can take data fed to some port, Amazon Kinesis , HDFS, Kafka, Flume and others.
Checkpointing - stores state to disk periodically for fault tolerance.

DStream - collection of distinct RDDs
val stream = new StreamingContext(conf,Seconds(1))
val lines = stream.socketTextStream("localhost",8888)
val errors = lines.filter(_.contains("error"))

kick off the job explicitly :
 stream.start()
 stream.awaitTermination()

Windowed Opertaions
updateStateByKey()

Need a twitter developer account  --- https://apps.twitter.com/

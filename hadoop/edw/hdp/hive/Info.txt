References:
================
 
 
Q&A
=========
Difference between external and internal tables
   http://stackoverflow.com/questions/17038414/difference-between-hive-internal-tables-and-external-tables
warehouse-dir vs target-dir
What is Managed table vs External table
Why CTAS does not support partitioning and clustered (bucketing) or skewed.

Bucketing is 1 based???

What is correlation between HBase and HCatalog?  
   http://stackoverflow.com/questions/19997749/what-is-correlation-between-hbase-and-hcatalog

Does Hive support indexing? How we can have faster searches?
     http://maheshwaranm.blogspot.com/2013/09/hive-indexing.html
      ex:  create INDEX test_column_index ON TABLE test_table(test_column) as
                 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'  WITH DEFERRED REBUILD;

How do we choose between partition vs bucketing ?

Did you use hive joins? in what scenario ?

can we create schema with primary key ? 

 SET hive.exec.dynamic.partition = true;
 
 SET hive.exec.dynamic.partition.mode = nonstrict;

Static vs Dynamic Partitioning ?
  https://www.altiscale.com/blog/best-practices-for-dynamic-partitioning-in-hive/
     Hive enforces a limit on the number of dynamic partitions it can create. 
     The default is 100 dynamic partitions per node, with a total (default) limit of 1000 dynamic partitions across all nodes.
     However, this setting is configurable. If your job tries to create more partitions than allowed, you may see the following exception:


copying schema to other cluster ??
    https://community.hortonworks.com/questions/4496/how-to-migrate-hive-data-over-to-new-cluster.html
    mysql -u hive -p -e " select concat(  'show create table ' , TBL_NAME,';') from TBLS" hive > file.sql
    
what are challenges you have faced in hive on your data ?
  1. unit date to hive representation
  2. Null presentations
  3. load issue to AWS S3 ?
     only external table is good. 
      http://stackoverflow.com/questions/30801690/issue-with-load-data-into-hive
  4. 
     
     
        
  


    





Key Points & Concepts
=====================

Types of partition in RDBMS - list partitioning, hash partitioning , range partitioning...
Hive Partition
 - Partitioning --- similar to list partioning
         partition column may be part of table.
         column name needs data type
 - Bucketing --- similar to has partioning
         Clustered or Sorted takes column from the table. Does require data type to be mentioned as it is already mentioned table.

hive.exec.dynamic.partition=true  ---> through insert dynamic partition can be done.
hive.exec.dynamic.partition.mode=nonstrict --> if strict, will throw error if trying to do dynamic partition.

CTAS - Create Table As Select --- creating table with a select query.

hive.execution.engine=mr   or tez

explain ... query --- gives information about the stages and map and reduce tasks.

hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager

To have transation behavior table needs to be orc , bucketing ,  tblproperties('transactional'='true'); 
compaction

tombstone
Vaccuming


--- compression parameters

[azure@sandbox ~]$ hive -e "set;"| grep compress
Logging initialized using configuration in file:/etc/hive/2.4.0.0-169/0/hive-log4j.properties
dfs.image.compress=false
dfs.image.compression.codec=org.apache.hadoop.io.compress.DefaultCodec
hive.exec.compress.intermediate=false
hive.exec.compress.output=false
hive.exec.orc.compression.strategy=SPEED
hive.exec.orc.default.compress=ZLIB

core-sites.xml
    <property>
      <name>io.compression.codecs</name>
      <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.SnappyCodec</value>
    </property>

mapred-sites.xml
   <property>
      <name>mapreduce.output.fileoutputformat.compress</name>
      <value>false</value>
    </property>

    <property>
      <name>mapreduce.output.fileoutputformat.compress.type</name>
      <value>BLOCK</value>
    </property>

compressed files named as *-deflate

set mapreduce.output.fileoutputformat.compress.codec;

We might need denormalize data - which can be used to get frequent different type of reports on the same set of denormalized data.





 
 
 

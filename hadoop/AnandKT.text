Extras:
===============
IDAA Netteza  --- 
Log Master --- goes through log archieve  
attunitty tool ---   check for new tool for web Interface.  http://www.attunity.com/


Project
=================

 401 plan ---
 Retail Data ---- consumer Retirement funds
                   mutual funds
				   equity  funds
				   
 Flagship programs
  
 
 Predictive models
    1. Clustered models-- (segmentation)
    2. Propensity models (predictions)
    3. Collaborative filtering(Recomendation)
	
 click stream analysis
  --- view event
  --- click events
  --- activity events
    site analysis



   
Tell me about your project

1. Data Consumption
 
2. We provide data ---

db2 --> staging data -- surrogate keys -->

sqoop --> nightly 500 million records -- 10 GB --- 4hrs
   client data  --- monthly
   demographic =  6 monthly
   equi
   
   un-adultrated data  
   
   data cleaned is and stored hive is called un-adultrated.
   
   edge node --- 



date --- Nov 27th
==========================

Staging -- raw data 

   spark to clean data

ETL ---- AVRO 
      --- more compatble with lot of platforms and technologies like java, c++
	  --- HIVE tables are built on it
	  --- its row format , performance impact so need to have parque format we have another copy in master 


archieve  --- ETL successful raw data 

master ---parque --- columnar data ---> 
   we can get only 10 columns out of 100 
   maintains dictionary 
   gets the data quickly to memory
   
AVRO vs Parquet 
     http://www.slideshare.net/StampedeCon/choosing-an-hdfs-data-storage-format-avro-vs-parquet-and-more-stampedecon-2015

AVRO to Parquet conversions
     http://blog.cloudera.com/blog/2015/03/converting-apache-avro-data-to-parquet-format-in-apache-hadoop/
     
   
RDW - Oracle db  
ENT - Mainframes db
NLU - Calls related db




sqoop
  - insert
  - update
  - upsert
  


prod - 30 nodes ---
     - 3 masters
	 - 
	 
scala	 
	traits  ... abstract

Techonology 
=============
  DB  -> Staging ==== sqoop
  Staging --> ETL ==== Spark/scala, Hive, AVRO format
  Staging --> Archieve === HDFS put
  ETL ---> Master  ==== spark/scala , Hive, Parquet format
  
  
  DMart
  Analytics ===  PySpark, Spark, Hive, SAS
  
  oozie 
  control-M 
  
TO-DO
======
Daily Tasks
Regular Problems
Technical snippets might be useful


Project --- QnA
================

1. How much data was received from third party?
   
   XYZ ????? (share of vallet)
	a. Quarterly data -- media,  EBS loads the data into shared location (FTP) 
   	b. 40 GB - Semi-structured data. flat file. (BA Clear --- any third party will send to BA clear).
   
   Google Stream
   	a. Click stream data
	b. google sends the file DIALY in batch mode.(300 GB) . Google Analytics ?
	    i. view
	    ii. click    20 GB
	    iii. Activity  1GB
	 Are we placing plain text data into the HDFS or compress data? 
	 Spark(1.1)/scala, MR, Pig, Hive has capability to deal with compressed/zip blocks????
	 Can we have sample goofed data for above scenarios?????
	 
	 http://support.sas.com/resources/papers/proceedings12/100-2012.pdf
	 
   
2. How it was received ? Who are the data providers?

   Data like. - 
    i. Person details
    ii. zip code, zipcode+4digitcode, agecode (External Demographic data).

3. What is the reason to use scala and spark other than performance ? 

4. How much data was processed ? 

5. What features of Spark are used in the Project and why?

6. Why impala and hive have been used together ?

7. How performance tuning was achieved when processing billions of record?

8. Was it a one batch job ? Multiple jibs? Hiw was the load distributed across cluster ?

9. How were the validatuon rules applied in what platform?

10. How we are processing click stream data using spark ?

11. What are technical challenges you have faced in your project ?
    a. Full Outer  joins  on Spark RDD and Hive ---  Issue is cluster hang. All resources are consumed (monitored on Job Server)
       Any actions ??? shuffling .... usually we allocate 20% of capacity to for shuffling.
       Shuffle area is getting utilized ... so running out of resources.
       This is scenario will when dynamic allocation is set true/enabled.
       i. Hive Map side (combiner) join helped in some sort in this scenario to recover from hung state.
       ii. Skewness ??? 
            Know the Executor capacity (memory and cores) . 
	    solution: Re-partition in spark.  (DataNode === worker) 
	              rdd.repartition(50)
       
	    
     b. Hive queries where taking lot time for execution - high latency is observed when joins on large data was performed.
        how do you observe high latency.
	solution: Map side joins.
     
     c. compression techniques used ? and why ?
        gzip , snappy
	        

12. 

